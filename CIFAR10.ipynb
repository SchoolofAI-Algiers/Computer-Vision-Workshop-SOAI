{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggFo2WqUClhq",
        "colab_type": "text"
      },
      "source": [
        "# Cifar10 \n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0SlR6usCvZ5",
        "colab_type": "text"
      },
      "source": [
        "### Loading the cifar10 dataset from torchvision datasets,the data set is devided to trainng set and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWjU8i6iCRBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "##TODO:\n",
        "# convert data to a normalized torch.FloatTensor using transform\n",
        "\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "# choose the training and test datasets\n",
        "##TODO : add the previous transform when you load data \n",
        "\n",
        "train_data = datasets.CIFAR10('data', train=True,\n",
        "                              download=True)\n",
        "test_data = datasets.CIFAR10('data', train=False,\n",
        "                             download=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpuuTb93D7r4",
        "colab_type": "text"
      },
      "source": [
        "### Dividing the traing set to a train set and validation set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Op-LJFrEFd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "##TODO:\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "# just specifying the image classes\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUT1qICFFAMy",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an7dXaCXFF85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "\n",
        "\n",
        "\n",
        "# obtain one batch of training images\n",
        "\n",
        "##TODO : iter need as a parmeter a the loader that you want to visualize \n",
        "\n",
        "dataiter = iter()\n",
        "\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGkNmuXyG39_",
        "colab_type": "text"
      },
      "source": [
        "### How can we visualize the channals of an image ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH-yJeazHIYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rgb_img = np.squeeze(images[3])\n",
        "channels = ['red channel', 'green channel', 'blue channel']\n",
        "\n",
        "fig = plt.figure(figsize = (36, 36)) \n",
        "for idx in np.arange(rgb_img.shape[0]):\n",
        "    ax = fig.add_subplot(1, 3, idx + 1)\n",
        "    img = rgb_img[idx]\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(channels[idx])\n",
        "    width, height = img.shape\n",
        "    thresh = img.max()/2.5\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "            ax.annotate(str(val), xy=(y,x),\n",
        "                    horizontalalignment='center',\n",
        "                    verticalalignment='center', size=8,\n",
        "                    color='white' if img[x][y]<thresh else 'black')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX2u2gHKHtdd",
        "colab_type": "text"
      },
      "source": [
        "## Creation the Convolutional Neural Network\n",
        "We need to implement :\n",
        "\n",
        "*   Convolutional layers \n",
        "*   Max-pooling layers \n",
        "*   Fully connected layers \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEHebgM0IpnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "##TODO:\n",
        "# define a model with multiple Convolutional layers,The more convolutional layers you include, the more complex patterns in color and shape a model can detect\n",
        "# but too many layers is also bad \n",
        "\n",
        "# define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #Defining the layers \n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Add sequence of convolutional and max pooling layers\n",
        "        #!!Dont forget the max pooling layers , dropout and the activation function  \n",
        "    \n",
        "    \n",
        "    \n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3kNeD75J_pn",
        "colab_type": "text"
      },
      "source": [
        "## Creation the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xecLc1e8KCzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a complete CNN\n",
        "model = Net()\n",
        "#showing the architecture of the CNN\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0JR4OhRKScw",
        "colab_type": "text"
      },
      "source": [
        "## Specifying the loss function : \n",
        "\n",
        "\n",
        "\n",
        "> What is a loss function ?\n",
        "\n",
        "> it’s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower number.-According to Algorithma- \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFR6iWwTKri9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = #TODO\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOCRZ4a1MUR8",
        "colab_type": "text"
      },
      "source": [
        "## Specifying the Optimizer : \n",
        "\n",
        "\n",
        "\n",
        "> What is an Optimizer ?\n",
        "\n",
        "> The process of changing the parameters (weights) of our model to try and minimize the loss function, and make our predictions as correct as possible,it links the loss function with the parameters of our model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6g7Fm3-Me1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify optimizer (we use SGD optimizer)\n",
        "#dont forget that optimizers accpetes as parameters the model parameters and the trainning rate  \n",
        "optimizer = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4E34p0QOaXH",
        "colab_type": "text"
      },
      "source": [
        "## Train the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb-tW3sJObsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specifiy the number of epochs to train the model\n",
        "n_epochs = ##TODO\n",
        "\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        " \n",
        "        ##TODO :forward pass: compute predicted outputs by passing inputs to the model\n",
        "       \n",
        "        ##TODO :calculate the loss\n",
        "        \n",
        "        ##TODO :backward pass: compute gradient of the loss with respect to model parameters\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        \n",
        "        ##TODO :forward pass: compute predicted outputs by passing inputs to the model\n",
        "        \n",
        "        ##TODO : calculate the loss\n",
        "       \n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    print('Epoch N° : {} \\tTraining Loss =: {:.6f} \\tValidation Loss =: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN_XlGpAPv9B",
        "colab_type": "text"
      },
      "source": [
        "## Test the Trained Network\n",
        "Dont worry it doesn't need to have 100% accuracy to be a GOOD model !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek-0V75aQER1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for data, target in test_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu-f74Z7QPyD",
        "colab_type": "text"
      },
      "source": [
        "## You visualize the result of your model by using this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VFvvJ6VQpQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(#TODO : insert your test loader)\n",
        "images, labels = dataiter.next()\n",
        "images.numpy()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if train_on_gpu:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images.cpu()[idx])\n",
        "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTxr1UZCROw7",
        "colab_type": "text"
      },
      "source": [
        "## Saving a model the loading it :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWPQsF9pRgK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##TODO : add a saving code when the validation loss deacrese in the training part \n",
        "\n",
        "\n",
        "\n",
        "##TODO : load the model with the best parameters from a saved one \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}